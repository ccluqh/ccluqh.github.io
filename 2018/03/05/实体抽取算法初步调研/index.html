<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>实体抽取算法初步调研 | Manan's Nar</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">实体抽取算法初步调研</h1><a id="logo" href="/.">Manan's Nar</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">实体抽取算法初步调研</h1><div class="post-meta"><a href="/2018/03/05/实体抽取算法初步调研/#comments" class="comment-count"></a><p><span class="date">Mar 05, 2018</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>参考链接：<a href="https://www.lookfor404.com/" target="_blank" rel="noopener">https://www.lookfor404.com/</a>  </p>
<p>&emsp;&emsp;命名实体（Named Entity ）主要包括实体（组织名，人名，地名）、时间表达式（日期、时间）和数字表达式（货币值、百分数等）。在对文档进行命名实体识别之前必须对文档进行分句，分词和词性标注。<br>&emsp;&emsp;目前已有的命名实体识别的方法主要分为两大类：基于规则的方法和基于统计的方法。目前国内有多个开源的中文语言处理工具可供直接调用实现命名实体识别，比如复旦大学研发的fudanNLP，中科院的NLPIR分词系统（又名ICTCLAS2013）和哈工大的LTP。Q:pyltp支持自定义命名实体识别吗?</p>
<p>&emsp;&emsp;基于规则的方法主要是根据待识别的命名实体的语言学上的表现形式，人为设定一些规则来识别命名实体的方法。这类方法实现的效果很大程度上依赖于规则的设定且需要大量的专业知识，而且因为不同领域内的实体具有不同的规则，所以对每个新领域的文本处理都要重新设定规则。使用基于规则的方法来进行命名实体识别比较消耗时间和消耗人力。  一般分为以下几个步骤：  </p>
<ul>
<li>分词</li>
<li>收集特征词词典，特征词指的是诸如公司，有限公司，管理局这些词语</li>
<li>对文本分词后的词语进行标注</li>
<li>根据标注进行正则表达式匹配</li>
</ul>
<p>&emsp;&emsp;基于统计的方法主要利用原始的或经过加工的（人工标注的）语料进行训练，其语料的加工（标注）不需要非常多的语言学的知识，而且小规模的语料可以在可接受的时间和人力代价下完成，且基于统计的方法实现的命名实体识别在新的领域使用时可以不作改动或者做较少的改动，只需要利用新领域的语料进行训练即可。但是由于基于统计的方法获取的概率知识不如基于规则的方法所具有的专家的语言学知识的可靠性，所以基于统计的命名实体识别系统的性能要比基于规则的命名实体识别的性能要低。用于命名实体识别的基于统计的方法主要有：N元模型、隐马尔克夫模型（HMM）、最大熵模型（ME）、决策树（Decision Tree）等等。目前评价性能最好的是隐马尔克夫模型。  </p>
<p>&emsp;&emsp;隐马尔可夫模型有三种应用场景，我们做命名实体识别只用到其中的一种——求观察序列的背后最可能的标注序列。即根据输入的一系列单词，去生成其背后的标注，从而得到实体。  </p>
<ul>
<li>N:状态的有限集合。在这里，是指每一个词语背后的标注。  </li>
<li>M:观察值的有限集合。在这里，是指每一个词语本身。</li>
<li>A:状态转移概率矩阵。在这里，是指某一个标注转移到下一个标注的概率。</li>
<li>B:观测概率矩阵，也就是发射概率矩阵。在这里，是指在某个标注下，生成某个词的概率。</li>
<li>π:初始概率矩阵。在这里，是指每一个标注的初始化概率。  </li>
</ul>
<p>这些元素，都是可以从训练语料集中统计出来的。训练语料如下：  </p>
<p>1.会议 B 163 C 107 A 10</p>
<p>意思是，会议这个词作为B标签出现了163次，作为C标签出现了107次，作为A标签出现了10次.  </p>
<p>2.标签转移矩阵<br><img src="https://www.lookfor404.com/wp-content/uploads/2018/02/nt.tr_.txt%E6%96%87%E4%BB%B6.png" alt="Alt text"><br>即，每一个标签转移到另一个标签的次数。比如第二行第四列的19945，代表着【A标签后面接着是C标签】出现了19945次。</p>
<p>最后，我们根据这些统计值，应用维特比（viterbi）算法，就可以算出词语序列背后的标注序列了。命名实体识别本质上就是序列标注，只需要自己定义好对应的标签以及模式串，就可以从标注序列中提取出实体块了。  </p>
<p>标注序列中哪些标注代表着实体呢？</p>
<p>HanLP作者整理了一个nt.pattern.txt，里面是所有可能是机构名的序列模式串（有点粗暴），然后用Aho-Corasick算法来进行匹配。具体的实现在OrgRecognize.py里面的get_organization，函数的作用是，输入原词语序列、识别出来的标注序列和序列模式串，输出识别出来的机构名实体。  </p>
<p>总结、待改进：  </p>
<p>&emsp;&emsp;用HMM来实现的命名实体识别算法，关键在于标签的自定义，你需要人工定义尽可能多的标签，然后在训练语料集里面自动标注这些标签，这也是最麻烦的地方。标注完语料集，生成HMM中的转移概率、初始概率、发射概率就很简单了，就是纯粹的统计。整个模型也没什么参数，用这些统计的数字即可计算。</p>
<p>算法可能可以改进的点如下：</p>
<p>1.针对命名实体的维特比（viterbi）算法中，如果遇到未登录词，默认发射概率为0。我们可以额外引入相似度机制来解决这个问题，比如利用同义词表或者词向量相似度，我们找到和未登录词相似、同时也在观测概率矩阵里面出现的词语，用这个词语的发射概率（或者对其乘一个缩放系数），来代替未登录词的发射概率。<br>2.初始化概率对最终效果的影响有待考证。因为初始化概率影响着单词序列第一个词的标注，假如，仅仅用发射概率来决定第一个词的标注，效果会不会更好？<br>HMM算法默认只考虑前一个状态（词）的影响，忽略了更多上下文信息（特征）。后来的MEMM、CRF，都是循序渐进的改进方法。传统机器学习方法里面，CRF是主流，下面介绍。  </p>
<p>&emsp;&emsp;用CRF（条件随机场）做命名实体识别：  </p>
<ul>
<li>定义实体标注集合：B:实体的开头 M:实体的中间部分 E:实体的结束 W:单独成实体 O:句子的其它成分  </li>
<li>训练文本、测试文本预处理：根据标注集合和额外四个特征对文本标注，注意训练文本要有正确的实体标注作为标签</li>
<li>测试文本的预处理和上面的基本一样，区别在于，测试文本没有正确的实体标注，所以测试文本的预处理文件只有五列。最后我们要用CRF模型预测的是第六列—标注</li>
<li>训练CRF模型，CRF模型的训练，需要一个特征模板，以便能够自动在训练文本中提取特征函数，特征模板的定义直接决定了最后的识别效果。可以采用CRF++这个开源工具包  </li>
</ul>
<p>&emsp;&emsp;在使用CRF模型之后，我们得到了不错的效果。线下训练文本的实体召回率可以达到91.3％，另外，识别出来的无效实体也少了很多。<br>&emsp;&emsp;当然，这个模型也不是完美的，比如，我们训练的这个模型就比较“看重”机构特征词。举个例子，如果“下属公司”单独出现，则它也可能会被识别为机构名，需要我们人工定义一些规则将其去除。  </p>
<p>&emsp;&emsp;利用双向lstm+CRF做命名实体识别：  </p>
<p>现在最主流最有效的方法基本上就是lstm+CRF了。其中CRF部分，只是把转移矩阵加进来了而已，而其它特征的提取则是交由神经网络来完成。当然了，特征提取这一部分我们也可以使用CNN，或者加入一些attention机制。最终只是想对句子里面的每一个词，分配一个标签。 </p>
<p>1.分配tag  </p>
<p>实体为LOC,PER,ORG和MISC，分别代表着地名，人名，机构名以及其他实体，其它词语会被标记为O。由于有一些实体（比如New York）由多个词组成，所以我们使用用一种简单的标签体系：B-来标记实体的开始部分，I-来标记实体的其它部分。我们最终只是想对句子里面的每一个词，分配一个标签。 </p>
<p>2.词向量表示  </p>
<ul>
<li>对于每一个单词，我们用词向量$w\in\mathbb{R}^n$来表示，用来捕获词本身的信息。这个词向量由两部分concat起来，一部分是用GloVe训练出来的词向量$w_{glove}\in \mathbb{R}^{d_1}$，另一部分，是字符级别的向量$w_{chars}\in\mathbb{R}^{d_2}$  </li>
<li>不需要人工提取特征，只需要字符级别上面使用双向LSTM，就可以提取到一些拼写层面的特征了。当然了，CNN或者其他的RNN也可以干类似的事情。  </li>
</ul>
<p><img src="https://www.lookfor404.com/wp-content/uploads/2018/02/Word-level-representation-from-characters-embeddings.jpg" alt=""></p>
<p>3.词的上下文信息表示  </p>
<ul>
<li>当有了词向量w之后，就可以对一个句子里的每一个词跑LSTM或者双向LSTM了，然后得到另一个向量表示：$h \in \mathbb{R}^k$</li>
<li>我们输入一个句子，有m个单词：$w_1, \ldots, w_m \in \mathbb{R}^n$，得到m个输出：$h_1, \ldots, h_m \in \mathbb{R}^k$。现在的输出，是包含上下文信息的  </li>
</ul>
<p><img src="https://www.lookfor404.com/wp-content/uploads/2018/02/%E5%88%A9%E7%94%A8%E5%8F%8C%E5%90%91LSTM%E6%8F%90%E5%8F%96%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF.jpg" alt="">  </p>
<p>4.解码，为单词分配tag<br>最后，我们要对每一个词分配一个tag。用一个全连接层就可以搞定。假如，一共有9种tag，那么我们可以得到权重矩阵$W \in \mathbb{R}^{9 \times k}$和偏置矩阵$b \in \mathbb{R}^9$，最后计算某个词的得分向量$s \in \mathbb{R}^9 = W \cdot h + b$,$s[i]$可以解释为，某个词标记成第i个tag的得分，有了分数之后，我们有两种方案用来计算最后的tag：  </p>
<ul>
<li>softmax：将得分归一化为概率。做了局部的考虑，也就是说，当前词的tag，是不受其它的tag的影响的。而事实上，当前词tag是受相邻词tag的影响的。</li>
<li>线性CRF：刻画相邻tag的依赖、转移关系，得出CRF得分式子  </li>
</ul>
<p>了解了CRF得分式子，接下来要做两件事： </p>
<ul>
<li>找到得分最高的tag序列。</li>
<li>计算句子的tag概率分布。</li>
</ul>
<p>5.训练  </p>
<p>6.使用模型  </p>
<p>另外，用多模态来做实体识别也是一个方向，特别是对于一些类似微博的语料（有图片），这样做效果更佳。</p>
<p>基于规则和词典的方法<br>基于规则的方法多采用语言学专家手工构造规则模板，选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词（如尾字）、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。</p>
<p>基于统计的方法<br>基于统计的方法利用人工标注的语料进行训练，标注语料时不需要广博的语言学知识，并且可以在较短时间内完成。基于统计机器学习的方法主要包括：隐马尔科夫模型（Hidden Markov Model, HMM）、最大熵（MaximunmEntropy, ME）（openNLP开源工具包）、支持向量机（Support Vector Machine, SVM）、条件随机场（ConditionalRandom Fields, CRF）等。</p>
<p>在这4种学习方法中，最大熵模型结构比较紧凑、具有较好的通用性，主要缺点是训练时间复杂性非常高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔科夫模型高一些，但是隐马尔科夫模型在训练和识别的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列的效率比较高。隐马尔科夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用，如短文本命名实体识别。</p>
<p>基于统计的方法对语料库的依赖比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。SIGHAN Bakeoff 08测评中，中文命名实体识别使用的语料主要包括：香港城市大学语料库（1 772 202 字，训练集）、微软亚洲研究院语料库（1 089 050 字，训练集）、北京大学语料库（1 833 177 字，训练集）。这些语料库比较小、因公不广泛，无法应用于大规模的NER系统。因此，目前的问题是如何最大限度地使用这些有限的语料库。</p>
<p>混合方法<br>自然语言处理并不完全是一个随机过程，单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：<br>（1）统计学习方法之间或内部层叠融合，如俞鸿魁等采用层叠隐马尔科夫模型对中文进行分词。<br>（2）规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。<br>（3）将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。这种方法在具体实现过程中需要考虑怎样高效地将两种方法结合起来，采用什么样的融合技术。由于命名实体识别在很大程度上依赖于分类技术，在分类方面可以采用的融合技术主要包括如Voting，XVoting， GradingVal，Grading等。  </p>
<p>Lin等将最大熵方法与基于词典匹配和规则模式的后处理相结合，前一阶段运行ME方法识别文本中的生物实体，第一阶段机器学习方法可能产生一定程度的边界识别错误和语义分类错误；通过第二阶段基于词典和规则模式匹配的后处理，修正实体边界并改进实体语义分类结果，提高了系统的准确率与召回率。</p>
<p>评测组织<br>目前，比较有影响力的评测会议主要有信息理解研讨会（Message Understanding Conference, MUC）、多语种实体评价任务（MultilingualEntity Task Evaluation, MET）、自动内容抽取（Automatic Content Extraction, ACE）、文本理解会议（DocumentUnderstanding Conference, DUG）、SIGHAN的Bakeoff评测等。<br>在MUC-7之后，MUC被由NIST主导的ACE评测取代。</p>
<p>实现方法<br>１．统计机器学习(HMM,CRF,MXENT)<br>２．深度学习(LSTM-CRF,)有篇Paper:Neural Architectures for Named Entity Recognition  </p>
<p>由于绝大部分的开源实现和已知的工业使用的都还是基于统计的方法，所以我选择使用统计的方法实现，且CRF是比较好的选择。</p>
</div><div class="tags"></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2018/03/08/哈工大ltp资料/" class="pre">哈工大ltp资料</a><a href="/2017/09/22/fast-ai-Deep-Learning-Lesson1/" class="next">fast.ai Deep Learning Lesson1</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/03/08/实体抽取算法调研总结/">实体抽取算法调研总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/08/深度学习实体抽取资料/">深度学习实体抽取资料</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/08/HanLP资料/">HanLP资料</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/08/斯坦福CoreNLP_NER资料/">斯坦福CoreNLP_NER资料</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/08/哈工大ltp资料/">哈工大ltp资料</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/05/实体抽取算法初步调研/">实体抽取算法初步调研</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/22/fast-ai-Deep-Learning-Lesson1/">fast.ai Deep Learning Lesson1</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/21/git-article/">git article to github</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/20/Hello-Hexo/">Hello-Hexo</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Luqh.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>